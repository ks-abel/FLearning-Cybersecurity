{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FLearning-Cybersecurity-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ks-abel/FLearning-Cybersecurity/blob/master/FLearning_Cybersecurity_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFkzLnP1gK0x"
      },
      "source": [
        "!pip install --quiet --upgrade tensorflow_federated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3T38ogF7h81"
      },
      "source": [
        "!pip install --quiet --upgrade nest_asyncio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FXTPgZ1gOjy"
      },
      "source": [
        "# Auteur : Komlan Sessofia\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "#tf.debugging.set_log_device_placement(True)\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "import tensorflow_federated as tff\n",
        "from tensorflow.python.data import Dataset\n",
        "from tensorflow.python.keras import regularizers\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl-Bin780cj7"
      },
      "source": [
        "# Chargement du dataset.\n",
        "CSVFile = \"https://flearning-cybersecurity.komlansessofia.com/resources/DataSet/mainSimulationAccessTraces.csv\"\n",
        "DS2OS_traffic_traces_DataFrame = pd.read_csv(CSVFile, sep=\",\")\n",
        "# Les attributs non négligeables\n",
        "feat = ['sourceType',\n",
        "        'sourceAddress',\n",
        "        'destinationServiceAddress',\n",
        "        'destinationServiceType',\n",
        "        'accessedNodeType',\n",
        "        'operation',\n",
        "        'value']\n",
        "# Les attributs négligeables \n",
        "Nfeat = ['sourceID',\n",
        "        'sourceLocation',\n",
        "        'destinationLocation',\n",
        "        'accessedNodeAddress',\n",
        "        'timestamp']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olB-nNmIgnB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1f51ff-969f-495d-92d1-e2314d1a09db"
      },
      "source": [
        "le = LabelEncoder()\n",
        "sc = StandardScaler()\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "BATCH_SIZE = 40\n",
        "REPEAT_NUM = 20\n",
        "SHUFFLE_BUFFER = 79\n",
        "PREFETCH_BUFFER = 40\n",
        "CLIENTS = 500\n",
        "\n",
        "!rm -R /tmp/logs/*\n",
        "train_logdir = \"/tmp/logs/scalars/training/\"\n",
        "train_summary_writer = tf.summary.create_file_writer(train_logdir)\n",
        "\n",
        "class_names = DS2OS_traffic_traces_DataFrame['normality'].unique()\n",
        "federated_dataset_spec = ''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/tmp/logs/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-O_7D4BgTg0"
      },
      "source": [
        "## Fonctions pour le prétraitement des données\n",
        "\n",
        "# Function to calculate missing values by column\n",
        "def missing_values_table(df):\n",
        "   \n",
        "    # Total missing values\n",
        "    mis_val = df.isnull().sum()\n",
        "    # Percentage of missing values\n",
        "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "    # Make a table with the results\n",
        "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "    # Rename the columns\n",
        "    mis_val_table_ren_columns = mis_val_table.rename(\n",
        "    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "    # Sort the table by percentage of missing descending\n",
        "    # .iloc[:, 1]!= 0: filter on missing missing values not equal to zero\n",
        "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "    '% of Total Values', ascending=False).round(2)  # round(2), keep 2 digits\n",
        "    # Print some summary information\n",
        "    print(\"Your slelected dataframe has {} columns.\".format(df.shape[1]) + '\\n' + \n",
        "    \"There are {} columns that have missing values.\".format(mis_val_table_ren_columns.shape[0]))\n",
        "    # Return the dataframe with missing information\n",
        "    return mis_val_table_ren_columns\n",
        "\n",
        "# Fonction pour la tranformation des attributs sous forme numérique\n",
        "def preprocess_features(DataFrame):\n",
        "  # Récupération des attributs non negligeables\n",
        "  DS2OS = DataFrame.drop(columns=['normality'], axis=1)\n",
        "  processed_features = DS2OS.copy()\n",
        "  # Conversion des variables catégorielles en numerique\n",
        "  processed_features[\"sourceAddress\"] = le.fit_transform(DS2OS['sourceAddress'])\n",
        "  processed_features[\"sourceType\"] = le.fit_transform(DS2OS['sourceType'])\n",
        "  processed_features[\"destinationServiceAddress\"] = le.fit_transform(DS2OS['destinationServiceAddress'])\n",
        "  processed_features[\"destinationServiceType\"] = le.fit_transform(DS2OS['destinationServiceType'])\n",
        "  processed_features[\"accessedNodeType\"] = le.fit_transform(DS2OS['accessedNodeType'])\n",
        "  processed_features[\"operation\"] = le.fit_transform(DS2OS['operation'])\n",
        "  return processed_features\n",
        "\n",
        "# Mise à l'échelle des attributs\n",
        "def scaling_features(fts):\n",
        "  df_norm = sc.fit_transform(fts)\n",
        "  features = pd.DataFrame(df_norm)\n",
        "  return features\n",
        "\n",
        "# Fonction pour la tranformation des etiquettes sous forme numérique\n",
        "def preprocess_targets(DataFrame):\n",
        "  output_targets = pd.DataFrame()\n",
        "  # Encodage des étiquettes cibles\n",
        "  output_targets[\"normality\"] = le.fit_transform(DataFrame['normality'])\n",
        "  return output_targets\n",
        "\n",
        "\n",
        "# Transformation de chaque instance en tenseur\n",
        "def numpy_line_to_tensor(numpy_lines):\n",
        "  tensors = []\n",
        "  for line in numpy_lines:\n",
        "    tensors.append(tf.constant(line))\n",
        "  return tensors\n",
        "\n",
        "\n",
        "# Fonctions pour le prétraitement des données d'entrée\n",
        "def preprocess(dataset):\n",
        "  def preprocess_transform(x, y):\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.cast(tf.reshape(x, [-1,7]), tf.float32),\n",
        "        y=tf.cast(tf.reshape(y, [-1,1]), tf.float32)\n",
        "    )\n",
        "  \n",
        "  return dataset.repeat(REPEAT_NUM\n",
        "              ).shuffle(SHUFFLE_BUFFER\n",
        "                        ).batch(BATCH_SIZE, drop_remainder=True\n",
        "                                ).map(preprocess_transform, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "                                      ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "# Fonction de creation et de répartition aléatoire des données sur les clients\n",
        "def generate_clients_datasets(n, source_x, source_y):\n",
        "  clients_dataset=[]\n",
        "  size = len(source_x)//n\n",
        "  for i in range(0, size*n, size):\n",
        "    dataset=tf.data.Dataset.from_tensor_slices((source_x[i:i+size], source_y[i:i+size]))\n",
        "    dataset=preprocess(dataset)\n",
        "    clients_dataset.append(dataset)\n",
        "  assert(len(clients_dataset) == n)\n",
        "  return clients_dataset\n",
        "\n",
        "# Tf preprocess\n",
        "def _preprocess_dataset(features, targets):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((features, targets))\n",
        "  dataset = preprocess(dataset)\n",
        "  return dataset\n",
        "\n",
        "# Création d'un modèle avec Keras\n",
        "def create_keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(7,)),\n",
        "      tf.keras.layers.Dense(7, activation='relu'),\n",
        "      tf.keras.layers.Dense(5, activation='relu'),\n",
        "      tf.keras.layers.Dense(8, activation='softmax')\n",
        "  ])\n",
        "\n",
        "\n",
        "# Construction d'un modèle pour TFF à partir du modèle Keras\n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=federated_dataset_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "  \n",
        "\n",
        "# Affichage des infos des metrics\n",
        "def display_metrics(metrics):\n",
        "  res = []\n",
        "  if 'train' in metrics:\n",
        "    for name, value in metrics['train'].items():\n",
        "      name = name.split(\"_\")\n",
        "      metric_name = name[len(name)-1]\n",
        "      res.append([metric_name, value])\n",
        "  else:\n",
        "    for name, value in metrics.items():\n",
        "      name = name.split(\"_\")\n",
        "      metric_name = name[len(name)-1]\n",
        "      res.append([metric_name, value])\n",
        "  return res\n",
        "\n",
        "\n",
        "# Processus d'évaluation (Test)\n",
        "def evaluation_process(model_fn, state, federated_test_dataset):\n",
        "  evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
        "  eval_metrics = evaluation(state.model, federated_test_dataset)\n",
        "  return eval_metrics\n",
        "\n",
        "\n",
        "# Processus de formation / Cycles de formation (entrainement & validation)\n",
        "def training_process(_state, federated_train_dataset, epoch=1):\n",
        "  global state\n",
        "  state = _state\n",
        "  for epoch_num in range(1, epoch+1):\n",
        "    separator = \": \"\n",
        "    #tm, vm = ([], ) * 2\n",
        "    tm = ([], )\n",
        "    import time\n",
        "    start = time.perf_counter()\n",
        "    state, train_metrics = iterative_process.next(state, federated_train_dataset)\n",
        "    #valid_metrics = evaluation_process(model_fn, state, federated_valid_dataset)\n",
        "    elapsed = time.perf_counter() - start\n",
        "    tm = display_metrics(train_metrics);\n",
        "    #vm = display_metrics(valid_metrics);\n",
        "    with train_summary_writer.as_default():\n",
        "      for t in tm:\n",
        "        tf.summary.scalar(t[0], t[1], step=epoch_num)\n",
        "    #with valid_summary_writer.as_default():\n",
        "     # for v in vm:\n",
        "      #  tf.summary.scalar(v[0], v[1], step=epoch_num)\n",
        "    tm.sort();\n",
        "    print('Elapsed %.3f s/tour' % elapsed)\n",
        "    print('Tour {:2d}/{:2d}, train= {}'.format(epoch_num, epoch, str(tm)))\n",
        "\n",
        "\n",
        "# Calcul de la moyenne fédérée \n",
        "def build_federated_averaging_process(model_fn, local_learning_rate, server_learning_rate=1.0):\n",
        "  favg_process = tff.learning.build_federated_averaging_process(\n",
        "              model_fn,\n",
        "              client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=local_learning_rate), # Pour le calculer les mises à jour du modèle local sur chaque client\n",
        "              server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=server_learning_rate) # Pour la mise à jour moyenne au modèle global sur le serveur\n",
        "          )\n",
        "  return favg_process\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgB0rluP3Dsl"
      },
      "source": [
        "# Nettoyage et gestion des valeurs manquantes\n",
        "DS2OS = DS2OS_traffic_traces_DataFrame.copy()\n",
        "DS2OS['accessedNodeType'] = DS2OS['accessedNodeType'].fillna('/malicious')\n",
        "DS2OS['value'] = DS2OS_traffic_traces_DataFrame['value'].fillna(0);\n",
        "DS2OS = DS2OS.replace({'value':{'false':'0', 'true':'1', 'twenty':'20', 'none':'0'}});\n",
        "DS2OS = DS2OS.replace({'value':{'org.+':'1'}},regex=True);\n",
        "DS2OS = DS2OS.drop(Nfeat, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJdhLD2fk9AE"
      },
      "source": [
        "# Transformation des données catégorielles et mise à l'échelle\n",
        "fts = preprocess_features(DS2OS)\n",
        "features = scaling_features(fts)\n",
        "targets = preprocess_targets(DS2OS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpZsvU4425Sp"
      },
      "source": [
        "# Création aléatoire des sous-ensembles des données de formation et de test\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size=0.3, stratify=targets, random_state=42)\n",
        "# Transformation en tableaux de chaque sous-ensemble\n",
        "DS2OS_train_features = x_train.to_numpy()\n",
        "DS2OS_train_labels = y_train.to_numpy()\n",
        "DS2OS_test_features = x_test.to_numpy()\n",
        "DS2OS_test_labels = y_test.to_numpy()\n",
        "# Transformation en tenseur de chaque ligne des sous-ensembles \n",
        "_DS2OS_train_features = numpy_line_to_tensor(x_train.to_numpy())\n",
        "_DS2OS_train_labels = numpy_line_to_tensor(y_train.to_numpy())\n",
        "_DS2OS_test_features = numpy_line_to_tensor(x_test.to_numpy())\n",
        "_DS2OS_test_labels = numpy_line_to_tensor(y_test.to_numpy())\n",
        "# Creation et répartition des données sur les clients (450)\n",
        "federated_train_dataset = generate_clients_datasets(CLIENTS, _DS2OS_train_features, _DS2OS_train_labels)\n",
        "federated_test_dataset = generate_clients_datasets(CLIENTS, _DS2OS_test_features, _DS2OS_test_labels)\n",
        "# type d'arguments attendus par le modèle\n",
        "federated_dataset_spec = federated_train_dataset[0].element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn18BzBm-ihP"
      },
      "source": [
        "# Apprentissage centralisé sur l'ensemble des données\n",
        "_model = create_keras_model()\n",
        "_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "print('\\nEntrainement')\n",
        "_history = _model.fit(DS2OS_train_features, DS2OS_train_labels, epochs=30, batch_size=BATCH_SIZE)\n",
        "print('\\nEvaluation finale')\n",
        "_mse = _model.evaluate(DS2OS_test_features, DS2OS_test_labels)\n",
        "#plot_history(_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTHPwfzj0jjP"
      },
      "source": [
        "# Matrice de confusion\n",
        "predictions = np.argmax(_model.predict(DS2OS_test_features), axis=-1)\n",
        "res = tf.math.confusion_matrix(DS2OS_test_labels, predictions)\n",
        "print('Confusion_matrix: ',res)\n",
        "\n",
        "# Rapport de classification\n",
        "print('\\nClassification Report\\n')\n",
        "print(classification_report(DS2OS_test_labels, predictions, target_names = [\"DoS\",\"DP\",\"MC\",\"MO\",\"SC\",\"SP\",\"WS\",\"NL\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKYIcL5Wm3-Y"
      },
      "source": [
        "# Apprentissage fédéré avec 1/4 des 500 clients\n",
        "train_datasets = [federated_train_dataset[node] for node in range(125)]\n",
        "test_datasets = [federated_test_dataset[node] for node in range(125)]\n",
        "# Définition du processus itératif qui effectue une moyenne fédérée sur les modeles clients\n",
        "iterative_process = build_federated_averaging_process(model_fn, 0.5, 1)\n",
        "# Construction de l'état initial du serveur\n",
        "state = iterative_process.initialize()\n",
        "training_process(state, train_datasets, epoch=30)\n",
        "eval_metrics = evaluation_process(model_fn, state, test_datasets)\n",
        "ev = display_metrics(eval_metrics)\n",
        "print('eval = {}'.format(str(ev)))\n",
        "\n",
        "# Matrice de confusion\n",
        "modelK = create_keras_model()\n",
        "modelK.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "state.model.assign_weights_to(modelK)\n",
        "predictions = np.argmax(modelK.predict(DS2OS_test_features), axis=-1)\n",
        "res = tf.math.confusion_matrix(DS2OS_test_labels, predictions)\n",
        "print('Confusion_matrix: ',res)\n",
        "\n",
        "# Rapport de classification\n",
        "print('\\nClassification Report\\n')\n",
        "print(classification_report(DS2OS_test_labels, predictions, target_names = [\"DoS\",\"DP\",\"MC\",\"MO\",\"SC\",\"SP\",\"WS\",\"NL\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl_b2TAYZwci"
      },
      "source": [
        "# Apprentissage fédéré avec 500 clients\n",
        "\n",
        "# Définition du processus itératif qui effectue une moyenne fédérée sur les modeles clients\n",
        "iterative_process = build_federated_averaging_process(model_fn, 0.5, 1)\n",
        "# Construction de l'état initial du serveur\n",
        "state = iterative_process.initialize()\n",
        "training_process(state, federated_train_dataset, epoch=30)\n",
        "eval_metrics = evaluation_process(model_fn, state, federated_test_dataset)\n",
        "ev = display_metrics(eval_metrics)\n",
        "print('eval = {}'.format(str(ev)))\n",
        "\n",
        "# Matrice de confusion\n",
        "modelK = create_keras_model()\n",
        "modelK.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "state.model.assign_weights_to(modelK)\n",
        "predictions = np.argmax(modelK.predict(DS2OS_test_features), axis=-1)\n",
        "res = tf.math.confusion_matrix(DS2OS_test_labels, predictions)\n",
        "print('Confusion_matrix: ',res)\n",
        "\n",
        "# Rapport de classification\n",
        "print('\\nClassification Report\\n')\n",
        "print(classification_report(DS2OS_test_labels, predictions, target_names = [\"DoS\",\"DP\",\"MC\",\"MO\",\"SC\",\"SP\",\"WS\",\"NL\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSirf-Ii16hp"
      },
      "source": [
        "!pip3 install ann_visualizer\n",
        "!pip install graphviz\n",
        "from ann_visualizer.visualize import ann_viz;\n",
        "ann_viz(_model, title=\"My first neural network\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}